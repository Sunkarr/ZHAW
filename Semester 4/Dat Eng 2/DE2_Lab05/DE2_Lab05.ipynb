{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Data Engineering 2: Lab 05\n---------------"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "487b2965-ec93-4e39-b0bc-06caf462c7eb",
     "inputWidgets": {},
     "title": ""
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Task 01: Transformation with map and flatMap\n###### Convert all words in a rdd to lowercase and split the lines of the document 'blogposts' using space."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "b394e68e-8e18-4d2e-9ed2-3c450a40034b",
     "inputWidgets": {},
     "title": ""
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# TODO: read the file blogtexts into an RDD rdd\n",
    "rdd = sc.textFile(\"/FileStore/tables/blogtexts\")\n",
    "\n",
    "# TODO: print the first 5 entries of the rdd\n",
    "rdd.take(5)\n"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "f6e6d11d-06b4-47a6-9726-f264ced53c1f",
     "inputWidgets": {},
     "title": ""
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# TODO: define a function which returns all tokens in lowercase\n",
    "def Func(lines):\n",
    "      lines = lines.lower()\n",
    "      return lines\n",
    "\n",
    "# TODO: apply the function to the RDD by using map and create a new RDD rdd1\n",
    "rdd1 = rdd.map(Func)\n",
    "\n",
    "# TODO: print the first 5 entries of the rdd1\n",
    "rdd1.take(5)\n"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "1805581b-aa8e-4e0c-9e8e-4911ff5e457c",
     "inputWidgets": {},
     "title": ""
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# TODO: apply the function to the RDD by using flatMap and create a new RDD rdd2\n",
    "rdd2 = rdd.flatMap(Func)\n",
    "\n",
    "# TODO: print the first 5 entries of the rdd2\n",
    "rdd2.take(5)"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "255e099b-d024-4669-954f-862268d2e883",
     "inputWidgets": {},
     "title": ""
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Task 02: Transformation with filter\n###### Remove all stopwords from the token list."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "e87d5aeb-4d25-4965-ab9b-3afa8d08b941",
     "inputWidgets": {},
     "title": ""
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# stopword list\n",
    "stopwords = ['is', 'am', 'are', 'the', 'for', 'a']\n",
    "\n",
    "# TODO: create a RDD rdd3 by using the filter method with just tokens which are not contained in the stopwords list\n",
    "rdd3 = (rdd1\n",
    "        .flatMap(lambda x: x.lower().split())  # Split into words and convert to lowercase\n",
    "        .filter(lambda x: x not in stopwords)   # Remove stopwords\n",
    "        )\n",
    "\n",
    "# TODO: print the first 5 entries of the rdd3\n",
    "rdd3.take(5)"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "f6b23bb9-4fce-4d1d-a183-e4f4bd55ce5e",
     "inputWidgets": {},
     "title": ""
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Task 03: Transformation with groupBy\n###### After getting the results into rdd3, we want to group the words in rdd3 based on which letters they start with. For example, suppose I want to group each word of rdd3 based on first 2 characters."
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "02cb863b-9c68-47b3-9871-86e1167f00c7",
     "inputWidgets": {},
     "title": ""
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# TODO: create a RDD rdd4 by using the groupBy method grouped based on the first 2 characters\n",
    "rdd4 = rdd3.groupBy(lambda w: w[0:2])\n",
    "\n",
    "# TODO: print the first 5 entries of the rdd4 with the corresponding grouped values\n",
    "print([(k, list(v)) for (k, v) in rdd4.take(5)])"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "ae7a6aa6-2e60-4dda-80f5-cc05c2375e10",
     "inputWidgets": {},
     "title": ""
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Task 04: Transformation with groupByKey / reduceByKey \n###### What if we want to calculate how many times each word is coming in corpus ?"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "0762bf65-128b-4f03-af0f-0769aae84a54",
     "inputWidgets": {},
     "title": ""
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# TODO: create a RDD rdd3_mapped from rdd3 with has the token as key and 1 as value\n",
    "rdd3_mapped = rdd3.map(lambda x: (x, 1))\n",
    "\n",
    "# TODO: create a RDD rdd3_grouped with is grouped by the key\n",
    "rdd3_grouped = rdd3_mapped.groupByKey()\n",
    "\n",
    "# TODO: print the first 5 entries of rdd3_grouped\n",
    "rdd3_grouped.take(5)\n",
    "\n",
    "# TODO: print the first 5 entries of the rdd3_grouped with the corresponding grouped values\n",
    "print(list((j[0], list(j[1])) for j in rdd3_grouped.take(5)))\n"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "a5b370f9-744b-46ff-a843-5753e1aa0ed6",
     "inputWidgets": {},
     "title": ""
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# TODO: create a RDD rdd3_freq_of_words with count as key and the words as value and sorted by the key\n",
    "rdd3_freq_of_words = rdd3_grouped.mapValues(sum).map(lambda x: (x[1],x[0])).sortByKey(False)\n",
    "\n",
    "# TODO: print the top 10 of rdd3_freq_of_words\n",
    "rdd3_freq_of_words.take(10)\n"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "18738567-0b55-485c-95d8-cc10e907623a",
     "inputWidgets": {},
     "title": ""
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# TODO: also try reduceByKey to produce the same result\n",
    "rdd3_freq_of_words = rdd3_mapped.reduceByKey(lambda x, y: x + y).map(lambda x: (x[1],x[0])).sortByKey(False)\n"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "fec25b30-b5a2-4b3a-b598-1734783f0c71",
     "inputWidgets": {},
     "title": ""
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Task 05: Parallelize a RDD to n partitions"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "86bf85fb-14f1-4791-bc11-313789be1053",
     "inputWidgets": {},
     "title": ""
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "# TODO: create an RDD rdd_dis5 with 5 as the number of partitions\n",
    "# TODO: np.arange(0, 30, 2) - give the list from 0 to 30 with step 2.\n",
    "rdd_dis5 = sc.parallelize(np.arange(0, 30, 2), 5)\n",
    "\n",
    "# TODO: print the RDD rdd_dis\n",
    "print(rdd_dis5.collect())\n",
    "\n",
    "# TODO: print the partitions of the RDD rdd_dis\n",
    "print(rdd_dis5.glom().collect())"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "8fa6b32c-2983-4863-bf35-ad49a7edb259",
     "inputWidgets": {},
     "title": ""
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# TODO: create an RDD rdd_dis2  with 2 as number of partitions\n",
    "# TODO: np.arange(0, 30, 2) - give the list from 0 to 30 with step 2.\n",
    "rdd_dis2 = sc.parallelize(np.arange(0, 30, 2), 2)\n",
    "\n",
    "# TODO: print the RDD rdd_dis\n",
    "print(rdd_dis2.collect())\n",
    "\n",
    "# TODO: print the partitions of the RDD rdd_dis\n",
    "print(rdd_dis2.glom().collect())\n"
   ],
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "showTitle": false,
     "cellMetadata": {},
     "nuid": "4e611156-769a-4446-af62-3ca8e4fe5939",
     "inputWidgets": {},
     "title": ""
    }
   },
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "mimetype": "text/x-python",
   "name": "python",
   "pygments_lexer": "ipython3",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.8.6",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "application/vnd.databricks.v1+notebook": {
   "notebookName": "DE2_Lab05",
   "dashboards": [],
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "language": "python",
   "widgets": {},
   "notebookOrigID": 2175610004198534
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
