{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "487b2965-ec93-4e39-b0bc-06caf462c7eb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Data Engineering 2: Graded Lab 02\n",
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4dcdaea9-0cf3-487b-9593-7a72f6db7ce0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Grading\n",
    "For this graded lab you can get a total of 20 points. These 20 points count 10% of your final grade for the course.\n",
    "\n",
    "#### Note\n",
    "Check each result carefully. Use data filter, cleaning, and transformation methods wherever needed. The data can sometimes be really messy and have hidden issues.\n",
    "\n",
    "#### Submission\n",
    "You are allowed to submit the solution in groups of **two or three** students.\n",
    "Submit your GradedLab02.ipynb file renamed to FirstnameStudent01LastnameStudent01_FirstnameStudent02LastnameStudent02_FirstnameStudent03LastnameStudent03.ipynb in moodle.   \n",
    "Please submit a runnable python jupyter notebook file.\n",
    "All other submissions will be rejected and graded with 0 points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "65d9aca7-9c50-44ad-9ab6-e59e96a304b5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Part 01: In this part of the graded lab you need to solve different tasks for analysing Twitter data (10 points)\n",
    "###### Note: The data for this part is contained in the part01 folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b394e68e-8e18-4d2e-9ed2-3c450a40034b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Task 01: Print the top10 words of the tweets (2 points)\n",
    "###### Read the 'text' column of the tweets from the file \"2023_01_01\" into an rdd and print the top10 words of the tweets. Note: lowercase all words and remove the stopwords from the stopwords list of the archive."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6e6d11d-06b4-47a6-9726-f264ced53c1f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "from pyspark.sql.functions import lower, explode, split, count, col, regexp_replace, length, trim\n",
    "\n",
    "# Read the JSON file\n",
    "tweets_2023_01_01 = spark.read.json(\"/FileStore/tables/data_january2023/2023_01_01.json\")\n",
    "\n",
    "# Read stopwords from file and collect as a Python list\n",
    "stopwords_df = spark.read.text(\"/FileStore/tables/stopwords.txt\")\n",
    "stopwords = [row.value.strip() for row in stopwords_df.collect()]\n",
    "\n",
    "# Remove URLs\n",
    "tweets_2023_01_01 = tweets_2023_01_01.withColumn(\n",
    "    \"text\",\n",
    "    regexp_replace(col(\"text\"), \"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\", \"\")\n",
    ")\n",
    "\n",
    "# Lowercase the text\n",
    "tweets_2023_01_01 = tweets_2023_01_01.withColumn(\"text\", lower(col(\"text\")))\n",
    "\n",
    "# Split text into words, remove punctuation, and explode into rows\n",
    "words_df = tweets_2023_01_01.select(\n",
    "    explode(\n",
    "        split(\n",
    "            regexp_replace(col(\"text\"), r\"[^a-zA-Z0-9]\", \" \"),  # Replace non-alphanumeric with space\n",
    "            r\"\\s+\"\n",
    "        )\n",
    "    ).alias(\"word\")\n",
    ")\n",
    "\n",
    "# Remove empty strings and stopwords\n",
    "words_df = words_df.withColumn(\"word\", trim(col(\"word\")))\n",
    "words_df = words_df.filter(\n",
    "    (length(col(\"word\")) > 0) &\n",
    "    (~col(\"word\").isin(stopwords))\n",
    ")\n",
    "\n",
    "# Count word frequencies and show top 10\n",
    "print(\"\\nTop 10 most frequent words:\")\n",
    "words_df.groupBy(\"word\").count().orderBy(col(\"count\").desc()).show(10)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e87d5aeb-4d25-4965-ab9b-3afa8d08b941",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Task 02: Find the user with the most tweets in January 2023 (2 points) \n",
    "###### Use paired RDDs and their functions to find the user with the most tweets in January 2023."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6b23bb9-4fce-4d1d-a183-e4f4bd55ce5e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read all JSON files as a DataFrame first\n",
    "tweets_df = spark.read.json(\"/FileStore/tables/data_january2023/*.json\")\n",
    "\n",
    "# Convert the DataFrame to an RDD\n",
    "tweets_rdd = tweets_df.rdd\n",
    "\n",
    "# Map to ((user_id, screen_name), 1) pairs\n",
    "user_screen_pairs = tweets_rdd.map(lambda row: ((row.user_id, row.screen_name), 1))\n",
    "\n",
    "# Reduce by key to count tweets per (user_id, screen_name)\n",
    "user_screen_counts = user_screen_pairs.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Find the (user_id, screen_name) with the most tweets\n",
    "most_tweets_user = user_screen_counts.reduce(lambda a, b: a if a[1] > b[1] else b)\n",
    "\n",
    "# Print the result\n",
    "print(\"\\nUser with the most tweets in January 2023:\")\n",
    "print(f\"User ID: {most_tweets_user[0][0]}, Screen Name: {most_tweets_user[0][1]}, Number of tweets: {most_tweets_user[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "02cb863b-9c68-47b3-9871-86e1167f00c7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Task 03: Print the top5 users with the most tweets in January 2023 including their top 5 terms (2 points)\n",
    "###### Use paired RDDs and their functions to find the users with the most tweets in January 2023. Afterwards analyse their text content of the tweets and print the top 5 terms from all their posted tweets. Note: lowercase all words and remove the stopwords from the stopwords list of the archive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae7a6aa6-2e60-4dda-80f5-cc05c2375e10",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Map to ((user_id, screen_name), 1) pairs\n",
    "user_screen_pairs = tweets_rdd.map(lambda row: ((row.user_id, row.screen_name), 1))\n",
    "\n",
    "# Reduce by key to count tweets per user\n",
    "user_screen_counts = user_screen_pairs.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Get the top 5 users with the most tweets\n",
    "top5_users = user_screen_counts.takeOrdered(5, key=lambda x: -x[1])\n",
    "\n",
    "# Read stopwords from file\n",
    "stopwords = set([row.value.strip().lower() for row in spark.read.text(\"/FileStore/tables/stopwords.txt\").collect()])\n",
    "\n",
    "# Remove stopwords and URL's\n",
    "def extract_words(text):\n",
    "    # Remove URL's\n",
    "    text = re.sub(r\"http[s]?://\\S+\", \"\", text)\n",
    "    # Lowercase and split into words\n",
    "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    # Remove stopwords\n",
    "    return [word for word in words if word not in stopwords]\n",
    "\n",
    "for user in top5_users:\n",
    "    user_id, screen_name = user[0]\n",
    "    # Filter tweets for this user\n",
    "    user_tweets = tweets_rdd.filter(lambda row: row.user_id == user_id)\n",
    "    # Extract and clean words from all their tweets\n",
    "    words_rdd = user_tweets.flatMap(lambda row: extract_words(row.text))\n",
    "    # Count word frequencies\n",
    "    word_counts = words_rdd.map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)\n",
    "    # Get top 5 terms\n",
    "    top5_terms = word_counts.takeOrdered(5, key=lambda x: -x[1])\n",
    "    # Print results\n",
    "    print(f\"\\nTop 5 terms for User ID: {user_id}, Screen Name: {screen_name}:\")\n",
    "    for word, count in top5_terms:\n",
    "        print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0762bf65-128b-4f03-af0f-0769aae84a54",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Task 04: Print minutewise the top5 terms within the tweets of the first of January (2 points)\n",
    "###### Find a solution of your choice to print the top5 terms within the tweets of the file \"2023_01_01\". Note: lowercase all words and remove the stopwords from the stopwords list of the archive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5b370f9-744b-46ff-a843-5753e1aa0ed6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lower, regexp_replace, split, explode, trim, length\n",
    "from pyspark.sql import Window\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Read tweets and stopwords\n",
    "tweets_df = spark.read.json(\"/FileStore/tables/data_january2023/2023_01_01.json\")\n",
    "stopwords = [row.value.strip().lower() for row in spark.read.text(\"/FileStore/tables/stopwords.txt\").collect()]\n",
    "\n",
    "# Remove URLs from text\n",
    "tweets_df = tweets_df.withColumn(\n",
    "    \"clean_text\",\n",
    "    regexp_replace(col(\"text\"), r\"http[s]?://\\S+\", \"\")\n",
    ")\n",
    "\n",
    "# Lowercase and split into words, remove punctuation\n",
    "tweets_df = tweets_df.withColumn(\n",
    "    \"clean_text\",\n",
    "    lower(col(\"clean_text\"))\n",
    ")\n",
    "tweets_df = tweets_df.withColumn(\n",
    "    \"word\",\n",
    "    explode(split(regexp_replace(col(\"clean_text\"), r\"[^a-zA-Z0-9]\", \" \"), r\"\\s+\"))\n",
    ")\n",
    "\n",
    "# Remove empty strings and stopwords\n",
    "tweets_df = tweets_df.withColumn(\"word\", trim(col(\"word\")))\n",
    "tweets_df = tweets_df.filter(\n",
    "    (length(col(\"word\")) > 0) &\n",
    "    (~col(\"word\").isin(stopwords))\n",
    ")\n",
    "\n",
    "# Extract minute from time\n",
    "tweets_df = tweets_df.withColumn(\"minute\", col(\"time\").substr(1, 16))  # \"2023-01-01T00:00\"\n",
    "\n",
    "# Count word frequencies per minute\n",
    "word_counts = tweets_df.groupBy(\"minute\", \"word\").count()\n",
    "\n",
    "# For each minute, get the top 5 words\n",
    "window = Window.partitionBy(\"minute\").orderBy(col(\"count\").desc())\n",
    "word_counts = word_counts.withColumn(\"rank\", F.row_number().over(window))\n",
    "top5_per_minute = word_counts.filter(col(\"rank\") <= 5)\n",
    "\n",
    "# Now, for each minute, create a DataFrame and store in a dictionary\n",
    "minutes = [row.minute for row in top5_per_minute.select(\"minute\").distinct().collect()]\n",
    "minute_dfs = {}\n",
    "\n",
    "for minute in minutes:\n",
    "    df = top5_per_minute.filter(col(\"minute\") == minute).select(\"minute\", \"word\", \"count\")\n",
    "    minute_dfs[minute] = df\n",
    "\n",
    "# Print the top5 words in tweets of every minute\n",
    "for minute in sorted(minute_dfs.keys()):\n",
    "    print(f\"\\nTop 5 words for minute {minute}:\")\n",
    "    minute_dfs[minute].show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "86bf85fb-14f1-4791-bc11-313789be1053",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Task 05: Your analysis (2 points)\n",
    "###### Find an interesting analysis question for the Twitter data and answer it with a solution of your choice. Explain shortly, why your question is interesting and what the value of your created information is."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Meine Idee ist anhand der Zeitzone, welche dem Timestamp immer zugeordnet ist, herauszufinden in welcher Region ein User höchst wahrscheinlich lebt.\n",
    "Dies ist relevant, weil man so ohne explizite Ortsangaben oder Geotags Rückschlüsse auf den geografischen Ursprung der Nutzer ziehen kann. Dadurch lassen sich zum Beispiel regionale Schwerpunkte und Zielgruppen besser identifizieren, Inhalte gezielter ausspielen oder auch ungewöhnliche Aktivitätsmuster erkennen, die auf automatisierte Accounts oder Bots hindeuten könnten."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Timezones\n",
    "\n",
    "offset_to_region = {\n",
    "    # North and South America (West to East)\n",
    "    \"-10:00\": \"Hawaii-Aleutian Time\",\n",
    "    \"-09:00\": \"Alaska Time\",\n",
    "    \"-08:00\": \"Pacific Time (US/Canada)\",\n",
    "    \"-07:00\": \"Mountain Time (US/Canada)\",\n",
    "    \"-06:00\": \"Central Time (US/Canada)\",\n",
    "    \"-05:00\": \"Eastern Time (US/Canada)\",\n",
    "    \"-04:00\": \"Atlantic Time (Canada)/Caribbean\",\n",
    "    \"-03:00\": \"Argentina/Brazil/Chile\",\n",
    "    \"-02:00\": \"Mid-Atlantic\",\n",
    "\n",
    "    # Europe and Africa (West to East)\n",
    "    \"+00:00\": \"UK/Ireland/Portugal (GMT)\",\n",
    "    \"+01:00\": \"Central European Time (Germany, France, Italy)\",\n",
    "    \"+02:00\": \"Eastern European Time (Finland, Greece, Egypt)\",\n",
    "    \"+03:00\": \"Moscow, East Africa\",\n",
    "\n",
    "    # Asia and Oceania (West to East)\n",
    "    \"+04:00\": \"Dubai, UAE, Azerbaijan\",\n",
    "    \"+05:00\": \"Pakistan, Kazakhstan\",\n",
    "    \"+05:30\": \"India, Sri Lanka\",\n",
    "    \"+06:00\": \"Bangladesh, Bhutan\",\n",
    "    \"+07:00\": \"Thailand, Vietnam, Indonesia\",\n",
    "    \"+08:00\": \"China, Singapore, Malaysia\",\n",
    "    \"+09:00\": \"Japan, South Korea\",\n",
    "    \"+09:30\": \"Central Australia\",\n",
    "    \"+10:00\": \"Eastern Australia\",\n",
    "    \"+11:00\": \"Solomon Islands\",\n",
    "    \"+12:00\": \"New Zealand, Fiji\"\n",
    "}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8fa6b32c-2983-4863-bf35-ad49a7edb259",
     "showTitle": false,
     "title": ""
    },
    "ExecuteTime": {
     "end_time": "2025-04-27T14:16:44.208389Z",
     "start_time": "2025-04-27T14:16:44.206641Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 6,
   "source": [
    "def analyze_user_timezone(user_id_input):\n",
    "    \"\"\"\n",
    "    Analysiert die Zeitzonen-Aktivität eines spezifischen Twitter-Users.\n",
    "\n",
    "    Args:\n",
    "        user_id_input (str): Die Twitter User ID\n",
    "\n",
    "    Returns:\n",
    "        None: Druckt die Analyse-Ergebnisse\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Lade alle Tweets aus Januar 2023\n",
    "        tweets_df = spark.read.json(\"/FileStore/tables/data_january2023/*.json\")\n",
    "        tweets_rdd = tweets_df.rdd\n",
    "\n",
    "        # Filtere Tweets des spezifischen Users\n",
    "        user_tweets = tweets_rdd.filter(lambda row: row.user_id == user_id_input)\n",
    "\n",
    "        # Prüfe ob User existiert\n",
    "        if user_tweets.count() == 0:\n",
    "            print(f\"Keine Tweets gefunden für User ID: {user_id_input}\")\n",
    "            return\n",
    "\n",
    "        # Hole Screen Name des Users\n",
    "        screen_name = user_tweets.first().screen_name\n",
    "\n",
    "        print(f\"\\nZeitzonenanalyse für User {screen_name} (ID: {user_id_input}):\")\n",
    "        print(f\"Gesamtzahl Tweets: {user_tweets.count()}\")\n",
    "\n",
    "        # Extrahiere Timezone Offsets\n",
    "        def extract_offset(row):\n",
    "            import re\n",
    "            match = re.search(r'([+-]\\d{2}:\\d{2})$', row.time)\n",
    "            return match.group(1) if match else None\n",
    "\n",
    "        offsets = user_tweets.map(extract_offset).filter(lambda x: x is not None)\n",
    "\n",
    "        # Zähle Häufigkeit der Timezones\n",
    "        offset_counts = offsets.map(lambda offset: (offset, 1)).reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "        # Finde häufigste Timezone\n",
    "        if not offset_counts.isEmpty():\n",
    "            most_common_offset = offset_counts.reduce(lambda a, b: a if a[1] > b[1] else b)\n",
    "\n",
    "            print(f\"\\nHäufigster Timezone Offset: {most_common_offset[0]}\")\n",
    "            print(f\"Anzahl Tweets in dieser Timezone: {most_common_offset[1]}\")\n",
    "            print(f\"Wahrscheinliche Region: {offset_to_region.get(most_common_offset[0], 'Unbekannte Region')}\")\n",
    "\n",
    "            # Zeige Verteilung aller Zeitzonen\n",
    "            print(\"\\nVerteilung aller Zeitzonen:\")\n",
    "            for offset, count in sorted(offset_counts.collect()):\n",
    "                region = offset_to_region.get(offset, \"Unbekannte Region\")\n",
    "                percentage = (count / user_tweets.count()) * 100\n",
    "                print(f\"Timezone {offset} ({region}): {count} Tweets ({percentage:.1f}%)\")\n",
    "        else:\n",
    "            print(\"Keine Timezone-Informationen gefunden in den Tweets.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Ein Fehler ist aufgetreten: {str(e)}\")\n",
    "\n",
    "# Beispiel-Nutzung:\n",
    "analyze_user_timezone(most_tweets_user[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7521f31-ba73-41bb-b926-df5ad98f5025",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Part 02: In this part of the graded lab you need to solve different tasks for analysing Graph data (10 points)\n",
    "\n",
    "###### Note: The data for this part is contained in the part02 folder. The library to import (like mentioned in Lab06 is also in the archive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b4a6fef4-9865-4ada-bdcd-c925f270045f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Task 01: Construct the graph (2 points)\n",
    "###### Create a graph with the structure you can find on the images in the part02 folder. Print ten items of the vertices and 10 of the edges.\n",
    "###### Note:  There is one image for vertices and one image for edges. You need to do some transformation to get to the desired result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3740b8a3-7ce7-4110-ad25-dea837bed22e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f1f2c304-8281-4357-869c-bd5bd95b8213",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Task 02: Motifs (2 points)\n",
    "###### Define a pattern which detects the all flights from New York with destination as Las Vegas and total delay of the flight should be no more than 60 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a3a17b4-19a6-4794-9d1d-dc3d2a433352",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78470cb5-cef2-47c0-9b01-62dbdb11afc6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Task 03: Graph Pattern Analysis (2 points)\n",
    "###### What are the flight routes with no direct connection?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eed0a82a-3504-4059-8b0c-cbefdf1b390e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f4d8df1-a87f-4f64-8be3-0559dd472e68",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Task 04: What are the most important airports, according to PageRank? (2 points)\n",
    "###### Note: Your allowed to use the pageRank function of the graph library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf0b2fd8-1c97-4eab-b484-ea6101466587",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f4e51fb2-61d6-4ffd-b397-61f9801a4915",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "##### Task 05: Your analysis (2 points)\n",
    "###### Find an interesting analysis question for the Graph data and answer it with a solution of your choice. Explain shortly, why your question is interesting and what the value of your created information is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "39c7c9d4-0314-4989-ae55-ef43516a5a34",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "DE2_GradedLab02",
   "notebookOrigID": 2059320195151653,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
